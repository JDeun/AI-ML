{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망(Artificial Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 정보를 어떻게 처리하고 학습하는지 궁금해 본 적이 있습니까? 예를 들어, 우리 몸은 손이나 다리를 움직일 수 있도록 정보를 어떻게 처리할까요? 간단하게 말하면 뇌에서 정보를 처리한 다음 몸의 다른 부분으로 신호를 보내 특정 근육의 움직임을 유도합니다. 이 신호는 신경계를 통해 전달됩니다. 신경계의 주요 구성 요소 중 하나는 뉴런 세포입니다. 이 세포들은 신호가 특정 값 또는 양보다 높은 경우에만 다른 세포로 신호를 전송합니다. 즉, 임계값을 기준으로 작동합니다. 따라서 우리가 손을 움직이기로 결정하면, 뇌의 신호는 다리 근육이 아닌 손 근육으로 뉴런 세포들을 통해 신호를 전달합니다.\n",
    "\n",
    "### 인공신경망 훈련\n",
    "\n",
    "그러나 이것은 우리가 정보를 처리하는 방법에 대해서만 설명해 줍니다. 인간의 학습 능력은 어떻게 작동하나요? 예를 들어, 우리는 빨간불에 멈춰서야 한다는 것을 알고 있고, 공을 차는 방법을 알고 있습니다. 다른 사람들이 하는 방법이나 사례를 보고 그렇게 하도록 훈련되었기 때문입니다. 이러한 예들을 통해 우리는 배우고 기억할 수 있었습니다.\n",
    "\n",
    "컴퓨터가 인간이 정보를 처리하고 학습하는 방식을 모방할 수 있다면 얼마나 좋을까요? 인공 신경망으로 이것이 가능합니다! 인공 신경망은 데이터 세트 내에서 복잡한 관계를 처리하고 '학습'할 수 있습니다. 간단한 신경망의 개념도는 아래와 같습니다.\n",
    "\n",
    "기본 아이디어는 입력 레이어(input layer)에 데이터를 입력하고, 뒤의 은닉 레이어(hidden layer)에서 데이터를 처리한다는 것입니다. 아래 그림에서 은닉 레이어가 하나만 표시되어 있지만 여러개의 은닉 레이어로 구성될 수  있습니다. 각 레이어는 데이터에 기능을 적용하고 다른 은닉 레이어로 전달한 후 최종적으로 출력 레이어(Output layer)로 끝나는 여러 개의 인공 뉴런으로 구성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"resources/ANN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 1개의 입력 레이어, 1개의 은닉 레이어(입력 레이어와 출력 레이어 사이) 및 1개의 출력 레이어로 구성된 간단한 신경망을 보여줍니다. 각 원은 1개의 노드 또는 1개의 뉴런을 나타냅니다. 입력 레이어는 모델에 전달되는 데이터일 뿐이므로 일반적으로 모델 아키텍처에서 입력 레이어의 노드 수는 이야기하지 않습니다. 위의 신경망 모델의 은닉 레이어는 2개의 노드/뉴런이 있고 출력 레이어에는 1개의 노드/뉴런이 있습니다.\n",
    "\n",
    "출력 레이어는 신경망의 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 어떻게 작동합니까? 기계 학습 프로젝트에 어떻게 유용할 수 있습니까? 인공 신경망에 대해 자세히 알아보려면 이 [동영상](https://www.youtube.com/watch?v=aircAruvnKk) 을 시청하세요. 영상을 일시 중지하고 신경망이 어떻게 작동하는지 이해하는 시간을 가져보세요. 워크시트에 신경망에 대해 흥미로운 정보를 기록해 두십시오. 5개의 노드가 가진 1개의 입력 레이어, 각각 3개의 노드가 있는 2개의 은닉 레이어, 2개의 노드가 있는 1개의 출력 레이어로 구성된 네트워크 그려보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망의 다양한 기능을 이해한 후에도 여전히 남아 있는 한 가지 의문점은 네트워크가 \"학습\"하는 방법입니다.\n",
    "\n",
    "3점 슛을 배우는 젊은 농구 선수를 생각해 봅시다. 슛을 하고 슛이 너무 짧아서 실패하면 농구 선수는 다음 슛의 강도를 높여서 거리감을 조정합니다. 다음 슛이 골대에서 너무 오른쪽으로 간다면 선수는 다음 슛이 골대 중앙을 향해 갈 수 있도록 방향을 조정합니다. 선수는 슛이 성공할 때까지 이 동작을 계속합니다. 이후 선수는 3점 슛을 쏠 때 성공했던 상황의 힘과 방향을 기억합니다.\n",
    "\n",
    "이것은 신경망이 훈련되는 방식과 유사합니다. 먼저 데이터가 네트워크를 통해 전달되고 예측된 출력이 제공됩니다. 이것을 순전파라고 합니다. 그런 다음 예측된 출력은 데이터의 실제 출력과 비교되고, 예측 출력과 실제 출력의 차이는 모델을 통해 뒤로 전달됩니다. 역방향으로 전파 동안 예측 출력과 실제 출력 간의 차이가 줄어들도록 모델 내에서 조정이 이루어집니다. 이것을 역전파라고 합니다. 조정이 이루어진 후, 데이터는 입력 레이어에서 다시 전달되고 또 순전파를 통해 다른 예측된 출력이 만들어집니다. 새로운 예측 출력은 실제 출력과 다시 비교되고 그 차이는 모델을 통해 역방향으로 다시 전달됩니다. 모델 내에서 더 많은 조정이 이루어지게 됩니다.\n",
    "\n",
    "예측 출력과 실제 출력의 차이가 최소화될 때까지 순전파와 역전파의 과정을 반복합니다. 최종적으로 모델이 학습되어 다른 유사한 데이터 세트의 예측에 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망이 훈련되는 방식과 비슷한 생활 속의 또 다른 예를 찾아볼 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 조금 더 이해할 수 있도록 다른 예를 살펴봅시다. 여러분은 아버지의 생일을 맞이하여 생일 케이크를 만드는 임무를 부여받았습니다. 요리책을 보고 기본적인 케이크를 만들 수 있습니다. 여러분은 지금까지 먹어본 케이크 중 가장 맛있는 케이크가 되길 바라는 마음에 엄마에게 시식을 부탁하러 갑니다. 여러분의 어머니는 케이크가 너무 달고 약간 탄거 같다고 대답합니다. 여러분은 다시 케이크를 만들면서 설탕의 양과 오븐에서 굽는 시간을 조정하려고 노력합니다. 새로운 레시피와 베이킹 시간으로 또 다른 케이크를 만든 다음에 어머님께 다시 맛 테스트를 요청할 것입니다. 이것은 여러분이 완벽한 케이크를 만들 때까지 계속될 것입니다.\n",
    "\n",
    "케이크를 굽는 첫 번째 단계는 인공 신경망 내에서 순전파 단계와 유사합니다. 어머니의 맛 테스트는 예상 출력과 실제 출력을 비교하는 것과 유사합니다. 설탕의 양과 베이킹 시간을 조정하는 것은 신경망 내에서 역전파와 유사합니다. 단계의 반복은 모델의 전체 학습 프로세스와 유사합니다.\n",
    "\n",
    "역전파의 작동 방식을 시각적으로 잘 이해하려면 이 [동영상](https://www.youtube.com/watch?v=Ilg3gGewQ5U) 을 보고 관심 있는 정보를 모두 기록해 두십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>보너스: 모델 내에서 조정되는 방법을 이해할 필요는 없습니다. 그러나 수학적으로 관심이 있거나 모든 조정을 이해하는 데 정말로 관심이 있고 시간이 있다면 아래 나열된 2개의 동영상을 시청해 보세요. 워크시트나 아래 셀에 관심있는 정보를 기록해 두십시오. </font>\n",
    "- [동영상 1](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [동영상 2](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower 데이터 세트를 이용한 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris Flower 데이터 세트를 사용하여 신경망을 훈련시켜 봅시다!\n",
    "\n",
    "## 1. 필수 라이브러리 가져오기\n",
    "먼저 필요한 라이브러리를 가져옵니다. pandas 및 numpy는 데이터 구조를 제공하고 scikit은 인공 신경망에 액세스하는 방법을 배울 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/PetalSepal1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 세트 획득 및 탐색\n",
    "\n",
    "Iris Flower 데이터 세트를 아래의 데이터 프레임 df로 가져옵니다.\n",
    "파일은 iris.data 입니다.\n",
    "\n",
    "모든 열의 헤더를 포함하는 것을 잊지 마십시오.\n",
    "\n",
    "변수를 이해하려면 위의 그림(출처: https://www.researchgate.net/Figure/Trollius-ranunculoide-flower-with-measured-traits_fig6_272514310) 을 참조하십시오.\n",
    "\n",
    "먼저 데이터를 탐색하겠습니다. 데이터 탐색이 어떻게 진행되었는지 기업하십니까?\n",
    "\n",
    "1. csv 파일을 열고 데이터 프레임에 넣습니다.\n",
    "2. 헤더를 포함합니다\n",
    "3. .info() 및 .describe()를 사용하여 데이터 세트에 대한 기본 정보를 확인합니다.\n",
    "\n",
    "누락된 값이나 오류 데이터가 있는지 확인합니다. 누락된 데이터가 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"[Dataset]_Module_18_(iris).data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = names\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 특성 및 목표 값 결정\n",
    "\n",
    "이제 데이터 세트를 x 값(모델이 관계를 학습할 수 있는 특성)과 y 값(목표 값 또는 모델의 예상 출력)으로 분할해야 합니다.\n",
    "\n",
    "### 표준화\n",
    "데이터 세트도 표준화해야 합니다.\n",
    "표준화의 용도는 무엇입니까? 이를 이해하려면 위의 데이터 분포를 살펴보십시오! 특성 데이터들은 서로 다른 평균과 표준 편차를 가지고 있습니다.이러한 변수를 비교하는 것은 어렵습니다. 표준화는 이러한 특성 데이터 들의 평균과 표준 편차를 균등화하여 쉽게 비교할 수 있도록 도와줍니다.\n",
    "\n",
    "scikit-learn 라이브러리의 스케일링 관련 [자료](http://benalexkeen.com/feature-scaling-with-scikit-learn/) 를 참조하십시오. 스케일링 전후에 데이터가 어떻게 변경되었는지 확인하십시오.\n",
    "\n",
    "이는 신경망이 쉽게 분류 작업을 할 수 있도록 하기 위한 것입니다. 아래 코드는 x 값을 x_value로 추출하고 표준화합니다. y_values는 나중에 추출합니다. 아래 코드를 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n"
     ]
    }
   ],
   "source": [
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "print(x_values.head())\n",
    "standardise = StandardScaler() # 표준척도는 분포의 평균값이 0이고 표준편차가 1이 되도록 데이터를 변환합니다.\n",
    "x_values = standardise.fit_transform(x_values)\n",
    "x_values_df = pd.DataFrame(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준화된 데이터 세트를 원본 데이터세트와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.900681  1.032057 -1.341272 -1.312977\n",
      "1 -1.143017 -0.124958 -1.341272 -1.312977\n",
      "2 -1.385353  0.337848 -1.398138 -1.312977\n",
      "3 -1.506521  0.106445 -1.284407 -1.312977\n",
      "4 -1.021849  1.263460 -1.341272 -1.312977\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".describe 함수를 사용하여 표준화된 데이터 세트의 현재 평균과 표준 값을 확인해 보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3\n",
      "count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02\n",
      "mean  -4.736952e-16 -6.631732e-16  3.315866e-16 -2.842171e-16\n",
      "std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00\n",
      "min   -1.870024e+00 -2.438987e+00 -1.568735e+00 -1.444450e+00\n",
      "25%   -9.006812e-01 -5.877635e-01 -1.227541e+00 -1.181504e+00\n",
      "50%   -5.250608e-02 -1.249576e-01  3.362659e-01  1.332259e-01\n",
      "75%    6.745011e-01  5.692513e-01  7.627586e-01  7.905908e-01\n",
      "max    2.492019e+00  3.114684e+00  1.786341e+00  1.710902e+00\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 신경망 구축\n",
    "\n",
    "이제 간단한 신경망을 구축해 봅시다. 그러기 위해서 keras 라이브러리에서 Dense와 Sequential 함수를 가져와야 합니다.\n",
    "\n",
    "### Sequential\n",
    "Sequential 모델을 사용하면 먼저 빈 모델 객체를 만든 다음 레이어를 순서대로 차례로 추가할 수 있습니다.\n",
    "\n",
    "### Dense\n",
    "Dense 레이어는 간단히 신경망의 뉴런 레이어라고 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gadi2\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 레이어 1개, 은닉 레이어 2개, 출력 레이어 1개로 신경망을 구축해 보겠습니다. \n",
    "은닉 레이어 내에 몇 개의 노드가 있어야 하는지를 결정하는 규칙은 없습니다. 이 신경망의 경우 각 은닉 레이어에 6개의 노드를 사용합니다.\n",
    "\n",
    "출력 레이어는 클래스 수만큼 노드를 사용해야 합니다. iris 데이터 세트의 경우 출력 레이어에 몇 개의 노드를 사용해야 합니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3502714414.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[106], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    3개입니다\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# your answer here\n",
    "3개입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 만드는 신경망을 그려보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw your neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망을 구축하기 위해 아래 코드를 실행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "\n",
    "# 6개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model.add(Dense(6, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선택 사항: [활성화 함수 간의 비교](http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/)\n",
    "\n",
    "이전 Acquire-CV에서 ReLu에 대해 학습하였습니다. 더 많은 활성화 함수가 있습니다. 특정 데이터/입력이 뉴런을 따라갈 수 있도록 하는 [on-off 버튼](https://en.wikipedia.org/wiki/Activation_function) 과 같습니다. 지금은 이 함수에 대하여 자세히 알 필요는 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일된 후에 모델의 요약된 정보를 출력할 수 있습니다. 아래 코드를 실행하여 확인하세요.\n",
    "- 모델의 레이어와 순서\n",
    "- 각 레이어의 출력 형태\n",
    "- 각 레이어의 매개변수(가중치) 수\n",
    "- 모델의 총 매개변수(가중치) 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m30\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_values는 원래 범주형 데이터이므로(숫자 대신 꽃 이름) 신경망을 훈련시키기 전에 범주의 y_value를 숫자로 변환하는 작업을해야 합니다. 신경망의 경우 범주가 숫자 그룹(예: 1,2,3,4 등)이 아닌 경우 원-핫 인코딩을 수행하기 전에 라벨 인코딩(이전 노트북을 참조하세요)을 먼저 수행해야 합니다.\n",
    "\n",
    "원-핫 인코딩에 대한 자세한 내용은 \"지도 학습 기술\" 노트북의 보너스 섹션을 참조하십시오. 이 [문서](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) 에서 더 자세히 알아볼 수도 있습니다. Keras의 to_categorical 함수를 사용하여 진행할 수 있습니다. 아래 코드를 실행하여 라벨 인코딩을 한 다음 y_values를 원-핫 인코딩하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: class, dtype: int64\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: class, dtype: int64\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 클래스의 데이터 포인트 수 출력\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# 각기 다른 클래스에 대하여 다른 숫자를 지정한 딕셔너리\n",
    "# 원-핫 인코딩으로 4개 열이 아닌 3개 열만 생성되도록 0부터 시작하는 값을 사용합니다.\n",
    "label_encode = {\"class\": {\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2}}\n",
    "\n",
    "# .replace를 사용하여 다른 클래스를 숫자로 변경\n",
    "df.replace(label_encode,inplace=True)\n",
    "\n",
    "# 각 클래스의 데이터 포인트 수를 출력하여 클래스가 숫자로 변경되었는지 확인\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# 클래스를 y_values로 추출\n",
    "y_values = df['class']\n",
    "\n",
    "# y_values 원-핫 인코딩\n",
    "y_values = to_categorical(y_values)\n",
    "\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 y_values에 나열된 내용을 확인하세요. 꽃 이름을 숫자로 인코딩했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 훈련시켜 봅시다. 아래 코드를 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2233 - loss: 1.2323  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2308 - loss: 1.2028 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2441 - loss: 1.2155 \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1874 - loss: 1.2109 \n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1795 - loss: 1.1930 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2086 - loss: 1.1664 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - accuracy: 0.2261 - loss: 1.1536\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2396 - loss: 1.1304 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.2518 - loss: 1.1041\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - accuracy: 0.2463 - loss: 1.1092\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3100 - loss: 1.0859 \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2841 - loss: 1.0799 \n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.3715 - loss: 1.0617\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3100 - loss: 1.0685 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.4233 - loss: 1.0350\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4777 - loss: 1.0045 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4461 - loss: 1.0102 \n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4454 - loss: 1.0118 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4755 - loss: 0.9999 \n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5200 - loss: 0.9915 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2870c6af010>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다.\n",
    "model.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 여러분의 첫 번째 신경망을 훈련했습니다. 정확도를 살펴보기 전에 사용되는 몇가지 용어를 이해해야 합니다.\n",
    "\n",
    "- Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "\n",
    "- us/step은 모델이 각 에포크에서 학습하는 데 걸린 시간을 보여줍니다.\n",
    "\n",
    "- acc는 모델이 얼마나 정확한지 보여줍니다.\n",
    "\n",
    "숫자가 각 epoch에 따라 어떻게 변경되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모델에서 얻은 정확도 값은 얼마입니까?\n",
    "\n",
    "모델에 다른 은닉 레이어를 추가하여 더 나은 정확도를 얻을 수 있는지 확인하십시오. \n",
    "추가 은닉 레이어는 이전 레이어와 동일한 수의 노드를 가질 수 있습니다.\n",
    "\n",
    "위에 나열된 관련 코드를 아래 셀에 복사하고 코드를 수정하여 은닉 레이어를 추가합니다. 새 모델을 훈련시키고 정확도가 향상되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m30\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135</span> (540.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m135\u001b[0m (540.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135</span> (540.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m135\u001b[0m (540.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model2 = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model2.add(Dense(6, input_dim=4, activation='relu'))\n",
    "\n",
    "# 6개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model2.add(Dense(6, activation='relu'))\n",
    "\n",
    "# 6개의 노드가 있는 세 번째 은닉 레이어를 추가합니다. \n",
    "model2.add(Dense(6, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model2.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4882 - loss: 1.0181  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5102 - loss: 0.9897 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5377 - loss: 0.9916 \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5557 - loss: 0.9659 \n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6216 - loss: 0.9411 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5656 - loss: 0.9582 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6278 - loss: 0.9180 \n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5724 - loss: 0.9564 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5620 - loss: 0.9306 \n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6023 - loss: 0.9174 \n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5850 - loss: 0.8995 \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5691 - loss: 0.8806 \n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6055 - loss: 0.8779 \n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6438 - loss: 0.8531 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6166 - loss: 0.8602 \n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6090 - loss: 0.8400 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6827 - loss: 0.7933 \n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6542 - loss: 0.7934 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7068 - loss: 0.7842 \n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7134 - loss: 0.7694 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2870f119fd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "model2.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 레이어를 추가한 후 정확도가 초기 모델보다 낮아졌습니다. 따라서 더 많은 레이어를 추가하는 것이 반드시 더 높은 정확도를 보장하는 것은 아닙니다.\n",
    "\n",
    "<font color = blue>보너스: 정확도를 향상시키기 위해 은닉 레이어의 노드 숫자를 늘려볼 수 있습니다. 은닉 레이어의 노드 숫자를 늘리면 정확도가 개선이 되나요?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2917 - loss: 1.3006  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3266 - loss: 1.2550 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3590 - loss: 1.2092 \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3756 - loss: 1.1443 \n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3770 - loss: 1.0946 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4098 - loss: 1.0688 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3957 - loss: 1.0309 \n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4950 - loss: 0.9755 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5036 - loss: 0.9472 \n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5534 - loss: 0.9321 \n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5549 - loss: 0.9078 \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6210 - loss: 0.8446 \n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5604 - loss: 0.8491 \n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6702 - loss: 0.8119 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6744 - loss: 0.8040 \n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7460 - loss: 0.7381 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.7143 - loss: 0.7108\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7510 - loss: 0.6902 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.7416 - loss: 0.6854\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7382 - loss: 0.6758 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">581</span> (2.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m581\u001b[0m (2.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">388</span> (1.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m388\u001b[0m (1.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model3 = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model3.add(Dense(10, input_dim=4, activation='relu'))\n",
    "\n",
    "# 10개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model3.add(Dense(10, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model3.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model3.fit(x_values,y_values,epochs=20,shuffle=True)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1859 - loss: 1.0947  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2297 - loss: 1.0633 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5461 - loss: 1.0158 \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.9822 \n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7803 - loss: 0.9387 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8126 - loss: 0.8906 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7995 - loss: 0.8614 \n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8053 - loss: 0.8025 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8080 - loss: 0.7798 \n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8397 - loss: 0.7197 \n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8142 - loss: 0.7195 \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8490 - loss: 0.6566 \n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8520 - loss: 0.6194 \n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8520 - loss: 0.5940 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8056 - loss: 0.5955 \n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8385 - loss: 0.5525 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8203 - loss: 0.5248 \n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8090 - loss: 0.5193 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8269 - loss: 0.5007\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8490 - loss: 0.4557 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28707493710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "\n",
    "# 50개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3366 - loss: 1.0065  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5828 - loss: 0.9404 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6584 - loss: 0.9024 \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6579 - loss: 0.8593 \n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6858 - loss: 0.8078 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7554 - loss: 0.7654 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7191 - loss: 0.7469 \n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7368 - loss: 0.7403 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.6698 \n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7771 - loss: 0.6550 \n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8237 - loss: 0.5836 \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8547 - loss: 0.5512 \n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8781 - loss: 0.5198 \n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8761 - loss: 0.4709 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8957 - loss: 0.4363 \n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8875 - loss: 0.4236 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8684 - loss: 0.4018\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8855 - loss: 0.3650 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8938 - loss: 0.3688 \n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8938 - loss: 0.3552 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28709e7f910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "\n",
    "# 100개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 우리는 신경망이 여러 Epoch 동안 훈련될 수 있음을 알았습니다. 이는 네트워크가 동일한 데이터 세트로 여러 번 계속 학습할 수 있음을 의미합니다. 모델이 동일한 데이터 세트로 계속 학습하면 어떻게 될까요? 이러한 과정으로 네트워크의 정확도가 높아질 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답변 : \n",
    "모델이 동일한 데이터 세트로 계속해서 학습한다면, 모델은 해당 데이터 세트에 대해 매우 높은 정확도를 가질 수 있습니다. 이는 모델이 데이터를 외워버리는 '과적합(overfitting)' 현상을 초래할 수 있습니다. 과적합은 모델이 훈련 데이터에만 최적화되어 새로운 데이터에 대한 일반화 성능이 저하되는 현상을 의미합니다. 즉, 모델이 학습 데이터에만 지나치게 의존하여 새로운 데이터에 대한 예측을 제대로 수행하지 못할 수 있습니다. 이러한 이유로 훈련 데이터에 대한 정확도가 높아지더라도, 실제 문제를 해결하기 위해서는 모델이 다양한 데이터를 학습하고 일반화할 수 있어야 합니다. 이를 위해 데이터의 다양성을 확보하고, 적절한 교차 검증과 같은 기술을 사용하여 과적합을 방지하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 계속 학습함에 따라 모델의 정확도가 증가할 것입니다. 동일한 데이터 세트로 시험해 볼 수 있습니다. 아래 코드를 실행하고 정확도를 관찰해 보세요. 동일한 설정으로 이전 모델보다 정확도가 높아졌습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4062 - loss: 1.1227  \n",
      "Epoch 2/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3850 - loss: 1.1441 \n",
      "Epoch 3/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3803 - loss: 1.1135 \n",
      "Epoch 4/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4664 - loss: 1.0822 \n",
      "Epoch 5/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3979 - loss: 1.1110 \n",
      "Epoch 6/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5159 - loss: 1.0794 \n",
      "Epoch 7/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5030 - loss: 1.0316 \n",
      "Epoch 8/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4874 - loss: 1.0629 \n",
      "Epoch 9/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5418 - loss: 1.0493 \n",
      "Epoch 10/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5370 - loss: 1.0154 \n",
      "Epoch 11/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5088 - loss: 1.0273 \n",
      "Epoch 12/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5241 - loss: 0.9988 \n",
      "Epoch 13/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5727 - loss: 0.9812 \n",
      "Epoch 14/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6265 - loss: 0.9790 \n",
      "Epoch 15/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6963 - loss: 0.9726 \n",
      "Epoch 16/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6634 - loss: 0.9425 \n",
      "Epoch 17/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7121 - loss: 0.9158 \n",
      "Epoch 18/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7444 - loss: 0.9159 \n",
      "Epoch 19/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7437 - loss: 0.9011 \n",
      "Epoch 20/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - accuracy: 0.7585 - loss: 0.8996\n",
      "Epoch 21/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7435 - loss: 0.8983 \n",
      "Epoch 22/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7599 - loss: 0.8769 \n",
      "Epoch 23/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7421 - loss: 0.8421 \n",
      "Epoch 24/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7774 - loss: 0.8342 \n",
      "Epoch 25/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.8283 \n",
      "Epoch 26/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7513 - loss: 0.8210 \n",
      "Epoch 27/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8144 - loss: 0.7764 \n",
      "Epoch 28/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7944 - loss: 0.7887 \n",
      "Epoch 29/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7992 - loss: 0.7768 \n",
      "Epoch 30/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7818 - loss: 0.7571 \n",
      "Epoch 31/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.7670 - loss: 0.7507\n",
      "Epoch 32/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8304 - loss: 0.7024\n",
      "Epoch 33/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7705 - loss: 0.7343 \n",
      "Epoch 34/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8000 - loss: 0.6901 \n",
      "Epoch 35/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7848 - loss: 0.6970 \n",
      "Epoch 36/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7904 - loss: 0.6893 \n",
      "Epoch 37/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7974 - loss: 0.6531 \n",
      "Epoch 38/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8117 - loss: 0.6317 \n",
      "Epoch 39/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8256 - loss: 0.6150 \n",
      "Epoch 40/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.7805 - loss: 0.6412\n",
      "Epoch 41/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8039 - loss: 0.6146 \n",
      "Epoch 42/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7957 - loss: 0.6222 \n",
      "Epoch 43/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8161 - loss: 0.6057\n",
      "Epoch 44/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7922 - loss: 0.5898 \n",
      "Epoch 45/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8096 - loss: 0.5929 \n",
      "Epoch 46/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8052 - loss: 0.5723 \n",
      "Epoch 47/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7749 - loss: 0.5769 \n",
      "Epoch 48/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.7983 - loss: 0.5828\n",
      "Epoch 49/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7979 - loss: 0.5579 \n",
      "Epoch 50/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7905 - loss: 0.5478 \n",
      "Epoch 51/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8079 - loss: 0.5316 \n",
      "Epoch 52/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7849 - loss: 0.5461 \n",
      "Epoch 53/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8080 - loss: 0.5212 \n",
      "Epoch 54/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8179 - loss: 0.4895 \n",
      "Epoch 55/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8088 - loss: 0.5132 \n",
      "Epoch 56/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8344 - loss: 0.4717\n",
      "Epoch 57/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8301 - loss: 0.4696 \n",
      "Epoch 58/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7767 - loss: 0.5239 \n",
      "Epoch 59/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8284 - loss: 0.4589 \n",
      "Epoch 60/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8197 - loss: 0.4392 \n",
      "Epoch 61/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8041 - loss: 0.4500 \n",
      "Epoch 62/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7989 - loss: 0.4646 \n",
      "Epoch 63/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8189 - loss: 0.4440 \n",
      "Epoch 64/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8054 - loss: 0.4548 \n",
      "Epoch 65/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8162 - loss: 0.4221 \n",
      "Epoch 66/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8155 - loss: 0.4508 \n",
      "Epoch 67/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8177 - loss: 0.4378 \n",
      "Epoch 68/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8260 - loss: 0.4607 \n",
      "Epoch 69/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8550 - loss: 0.3963 \n",
      "Epoch 70/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.4274 \n",
      "Epoch 71/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - accuracy: 0.8511 - loss: 0.4036\n",
      "Epoch 72/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8021 - loss: 0.4349 \n",
      "Epoch 73/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8295 - loss: 0.4233 \n",
      "Epoch 74/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8425 - loss: 0.4138 \n",
      "Epoch 75/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4381 \n",
      "Epoch 76/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8130 - loss: 0.4183 \n",
      "Epoch 77/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8222 - loss: 0.4018\n",
      "Epoch 78/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.3839 \n",
      "Epoch 79/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.3784 \n",
      "Epoch 80/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.4140 \n",
      "Epoch 81/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8505 - loss: 0.3814 \n",
      "Epoch 82/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8440 - loss: 0.3887 \n",
      "Epoch 83/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8548 - loss: 0.3760 \n",
      "Epoch 84/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8639 - loss: 0.3554\n",
      "Epoch 85/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8618 - loss: 0.3440 \n",
      "Epoch 86/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8535 - loss: 0.3540 \n",
      "Epoch 87/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.3401 \n",
      "Epoch 88/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8713 - loss: 0.3521 \n",
      "Epoch 89/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8531 - loss: 0.3532 \n",
      "Epoch 90/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8817 - loss: 0.3289 \n",
      "Epoch 91/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.8791 - loss: 0.3395\n",
      "Epoch 92/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8717 - loss: 0.3498 \n",
      "Epoch 93/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8635 - loss: 0.3287 \n",
      "Epoch 94/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8514 - loss: 0.3408 \n",
      "Epoch 95/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8518 - loss: 0.3339 \n",
      "Epoch 96/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8974 - loss: 0.3017 \n",
      "Epoch 97/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8696 - loss: 0.3212 \n",
      "Epoch 98/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8671 - loss: 0.3260 \n",
      "Epoch 99/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 0.3319 \n",
      "Epoch 100/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8810 - loss: 0.3201 \n",
      "Epoch 101/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9027 - loss: 0.2965\n",
      "Epoch 102/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.8853 - loss: 0.3021\n",
      "Epoch 103/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8680 - loss: 0.3091 \n",
      "Epoch 104/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9049 - loss: 0.2680 \n",
      "Epoch 105/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8940 - loss: 0.3094 \n",
      "Epoch 106/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8580 - loss: 0.3182 \n",
      "Epoch 107/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8729 - loss: 0.3011 \n",
      "Epoch 108/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8951 - loss: 0.2945 \n",
      "Epoch 109/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9107 - loss: 0.2732 \n",
      "Epoch 110/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9263 - loss: 0.2430 \n",
      "Epoch 111/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8920 - loss: 0.3122 \n",
      "Epoch 112/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9220 - loss: 0.2648 \n",
      "Epoch 113/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9115 - loss: 0.2737\n",
      "Epoch 114/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9037 - loss: 0.2692 \n",
      "Epoch 115/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9137 - loss: 0.2778 \n",
      "Epoch 116/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8846 - loss: 0.2827 \n",
      "Epoch 117/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9085 - loss: 0.2538 \n",
      "Epoch 118/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8846 - loss: 0.2841 \n",
      "Epoch 119/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9312 - loss: 0.2459 \n",
      "Epoch 120/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9087 - loss: 0.2760 \n",
      "Epoch 121/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.2421 \n",
      "Epoch 122/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9083 - loss: 0.2976 \n",
      "Epoch 123/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9413 - loss: 0.2671 \n",
      "Epoch 124/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9096 - loss: 0.2709 \n",
      "Epoch 125/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9478 - loss: 0.2329\n",
      "Epoch 126/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9256 - loss: 0.2510 \n",
      "Epoch 127/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9213 - loss: 0.2642 \n",
      "Epoch 128/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2566 \n",
      "Epoch 129/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9161 - loss: 0.2665 \n",
      "Epoch 130/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9070 - loss: 0.2759 \n",
      "Epoch 131/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9196 - loss: 0.2526\n",
      "Epoch 132/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9057 - loss: 0.2597\n",
      "Epoch 133/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9365 - loss: 0.2289 \n",
      "Epoch 134/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2416 \n",
      "Epoch 135/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9157 - loss: 0.2511 \n",
      "Epoch 136/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9209 - loss: 0.2546 \n",
      "Epoch 137/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9391 - loss: 0.2331 \n",
      "Epoch 138/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9084 - loss: 0.2859 \n",
      "Epoch 139/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9214 - loss: 0.2441 \n",
      "Epoch 140/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9505 - loss: 0.2074 \n",
      "Epoch 141/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9392 - loss: 0.2420 \n",
      "Epoch 142/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9414 - loss: 0.2117 \n",
      "Epoch 143/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.2015 \n",
      "Epoch 144/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.2098 \n",
      "Epoch 145/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9370 - loss: 0.1941\n",
      "Epoch 146/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9140 - loss: 0.2336 \n",
      "Epoch 147/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9488 - loss: 0.2053 \n",
      "Epoch 148/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9427 - loss: 0.2239 \n",
      "Epoch 149/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9648 - loss: 0.1813 \n",
      "Epoch 150/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9414 - loss: 0.1945 \n",
      "Epoch 151/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9284 - loss: 0.2182 \n",
      "Epoch 152/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9340 - loss: 0.2073 \n",
      "Epoch 153/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2008 \n",
      "Epoch 154/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9540 - loss: 0.1856 \n",
      "Epoch 155/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9327 - loss: 0.1999 \n",
      "Epoch 156/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9370 - loss: 0.2182\n",
      "Epoch 157/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9340 - loss: 0.2001 \n",
      "Epoch 158/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9444 - loss: 0.1968 \n",
      "Epoch 159/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9579 - loss: 0.1708 \n",
      "Epoch 160/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9314 - loss: 0.1956\n",
      "Epoch 161/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9205 - loss: 0.2152 \n",
      "Epoch 162/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9262 - loss: 0.2017 \n",
      "Epoch 163/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9492 - loss: 0.1761 \n",
      "Epoch 164/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1903 \n",
      "Epoch 165/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9392 - loss: 0.1914 \n",
      "Epoch 166/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9488 - loss: 0.1670\n",
      "Epoch 167/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9496 - loss: 0.1648 \n",
      "Epoch 168/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9444 - loss: 0.1668 \n",
      "Epoch 169/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9570 - loss: 0.1533 \n",
      "Epoch 170/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9288 - loss: 0.1822 \n",
      "Epoch 171/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9652 - loss: 0.1477 \n",
      "Epoch 172/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9327 - loss: 0.1699 \n",
      "Epoch 173/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.1496 \n",
      "Epoch 174/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9440 - loss: 0.1723 \n",
      "Epoch 175/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9318 - loss: 0.1682 \n",
      "Epoch 176/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9349 - loss: 0.1755 \n",
      "Epoch 177/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9427 - loss: 0.1580 \n",
      "Epoch 178/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9288 - loss: 0.1653 \n",
      "Epoch 179/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9496 - loss: 0.1501 \n",
      "Epoch 180/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9192 - loss: 0.1828 \n",
      "Epoch 181/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9579 - loss: 0.1463 \n",
      "Epoch 182/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9414 - loss: 0.1517 \n",
      "Epoch 183/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9409 - loss: 0.1540 \n",
      "Epoch 184/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9414 - loss: 0.1576 \n",
      "Epoch 185/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9362 - loss: 0.1557 \n",
      "Epoch 186/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1487 \n",
      "Epoch 187/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9440 - loss: 0.1392 \n",
      "Epoch 188/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9448 - loss: 0.1462 \n",
      "Epoch 189/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1590 \n",
      "Epoch 190/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9409 - loss: 0.1515 \n",
      "Epoch 191/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9479 - loss: 0.1370 \n",
      "Epoch 192/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9492 - loss: 0.1428 \n",
      "Epoch 193/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9418 - loss: 0.1414 \n",
      "Epoch 194/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1382 \n",
      "Epoch 195/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9414 - loss: 0.1434 \n",
      "Epoch 196/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9392 - loss: 0.1550 \n",
      "Epoch 197/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9197 - loss: 0.1607 \n",
      "Epoch 198/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9649 - loss: 0.1210\n",
      "Epoch 199/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1336 \n",
      "Epoch 200/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9505 - loss: 0.1352 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28711b7ec10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model4 = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model4.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# 6개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model4.add(Dense(6,activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다.\n",
    "model4.fit(x_values,y_values,epochs=200,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 점수가 이제 90% 이상임을 출력에서 확인할 수 있습니다. 에포크 수를 늘리는 것만으로도 정확도를 높일 수 있습니다. 모델이 훈련된 데이터에 대해서만 정확하다는 것이 좋다고 생각하십니까? \n",
    "축구 선수가 경기장의 특정 지점에서 골을 넣는 것만 연습한다면 경기에서도 좋은 결과를 얻을 수 있을까요? 제빵사가 특정한 맛, 모양, 크기의 케이크를 굽는 방법만 배운다면 고객의 요청에 따라 맛있는 케이크를 만들 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축구 선수는 운동장의 다른 지역에서는 정확하게 슛을 쏘지 못할 수 있으므로 경기에서 좋은 결과를 얻기 어려울 수 있습니다. 제빵사는 한 가지 특정 맛만 알기 때문에 손님이 원하는 맛있는 케이크를 구울 수 없습니다.\n",
    "\n",
    "이 질문의 이면에 있는 아이디어는 과적합에 대한 개념입니다. 모델이 과적합되면 새로운 데이터에 대하여 예측 성능이나 정확도가 떨어질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 과적합의 개념이며 모든 기계 학습 기술에도 적용됩니다. 모델이 데이터 세트에 너무 정확하게 맞춰지면 훈련된 모델은 이전에 본 적이 없는 새로운 데이터에 대하여 일반화하여 사용할 수 없습니다. 따라서 우리는 일반적으로 보유하고 있는 데이터 세트의 일부에 대해서만 모델을 훈련하고 나머지는 모델이 과적합되었는지 확인하기 위해 테스트 또는 검증 데이터 세트로 유지해야 합니다. Epoch가 증가함에 따라 훈련 세트로 학습시킨 모델의 테스트 세트의 정확도는 높아져야 합니다. 그러나 과적합 지점에서 테스트 세트의 정확도가 감소하기 시작합니다. 특정 Epoch 이후에 테스트 정확도가 증가하기 시작했다면 더 이상 모델을 훈련해서는 안 됩니다.\n",
    "\n",
    "여기에서 사용하는 데이터 세트를 훈련 세트와 테스트 세트로 나누어 적용해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 모델이 학습에 노출되지 않고 유지할 데이터의 양을 결정해야 합니다. 일반적으로 전체 데이터의 20~30%를 테스트 세트로 유지합니다. 이 예에서는 데이터 세트의 25%를 테스트/검증 세트로 유지합니다. sklearn 라이브러리에서 train_test_split 함수를 합니다. 아래 코드를 실행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train: 112\n",
      "Number of rows in x_test: 38\n",
      "Number of rows in y_train: 112\n",
      "Number of rows in y_test: 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 프레임 df에서 원래 x_values를 추출합니다.\n",
    "# 표준화는 모델이 학습할 데이터에만 기반해야 하므로 x_values를 다시 추출해야 합니다.\n",
    "# 따라서 표준화하기 전에 먼저 데이터를 분할해야 합니다.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.25는 전체 데이터의 25%가 x_test 및 y_test로 배정되면75%는 x_train 및 y_train에 배정됨을 나타냅니다.\n",
    "# random_state=10은 아래 코드를 실행할 때마다 분할이 동일하도록 하는 데 사용됩니다.\n",
    "# 분할은 매번 랜덤으로 하기 때문입니다. 동일한 random_state는 매번 동일하게 분할되도록 보장하는 유일한 방법입니다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n",
    "\n",
    "# x_train, x_test, y_train 및 y_test의 행 수 확인\n",
    "print(\"Number of rows in x_train:\", x_train.shape[0])\n",
    "print(\"Number of rows in x_test:\", x_test.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# 이제 x 값을 표준화할 수 있습니다.\n",
    "# StandardScaler 초기화\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# .fit_transform을 사용하여 x_train 값을 표준화합니다.\n",
    "x_train = standardise.fit_transform(x_train)\n",
    "\n",
    "# .transform을 사용하여 x_test 값을 표준화합니다.\n",
    "# 표준화는 x_train과 같아야 하므로 데이터를 맞출 필요가 없습니다.\n",
    "x_test = standardise.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 세트의 20%를 테스트/검증 세트로 유지하여 데이터 세트를 훈련 및 테스트/검증 세트로 분할하는 코드를 작성하십시오. \n",
    "x_train2, x_test2, y_train2 및 y_test2를 변수로 사용하십시오. 각 변수에 대한 행 수가 올바른지 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train2: 120\n",
      "Number of rows in x_test2: 30\n",
      "Number of rows in y_train2: 120\n",
      "Number of rows in y_test2: 30\n"
     ]
    }
   ],
   "source": [
    "# 데이터 세트의 20%를 테스트/검증 세트로 유지하여 데이터 세트를 훈련 및 테스트/검증 세트로 분할\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_values,y_values,test_size=0.2,random_state=10)\n",
    "\n",
    "# x_train, x_test, y_train 및 y_test의 행 수 확인\n",
    "print(\"Number of rows in x_train2:\", x_train2.shape[0])\n",
    "print(\"Number of rows in x_test2:\", x_test2.shape[0])\n",
    "print(\"Number of rows in y_train2:\", y_train2.shape[0])\n",
    "print(\"Number of rows in y_test2:\", y_test2.shape[0])\n",
    "\n",
    "# 이제 x 값을 표준화할 수 있습니다.\n",
    "# StandardScaler 초기화\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# .fit_transform을 사용하여 x_train 값을 표준화합니다.\n",
    "x_train2 = standardise.fit_transform(x_train2)\n",
    "\n",
    "# .transform을 사용하여 x_test 값을 표준화합니다.\n",
    "# 표준화는 x_train과 같아야 하므로 데이터를 맞출 필요가 없습니다.\n",
    "x_test2 = standardise.transform(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 x_train2, x_test2, y_train2 및 y_test2를 사용하여 신경망을 훈련시킵니다. 각각 6개의 노드가 있는 2개의 은닉 레이어와 3개의 노드가 있는 1개의 출력 레이어가 있는 신경망을 만드는 코드를 작성하십시오. 처음 은닉 레이어의 활성화(activation)는 'relu'인 반면 출력 레이어에 대한 활성화는 'softmax'입니다. 사용할 옵티마이저(optimizer)는 'adam'이고 손실(loss)은 'categorical_crossentropy'여야 합니다. 측정 항목(metrics)는 '정확도(accuracy)'입니다. model_val을 모델 변수로 사용하십시오. 모델을 컴파일한 후 모델 요약을 출력하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m30\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 새로운 Sequential 모델을 초기화합니다.\n",
    "model_val = Sequential()\n",
    "\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model_val.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# 두 번째 은닉 레이어를 추가합니다. 6개의 노드와 'relu' 활성화 함수를 사용합니다.\n",
    "model_val.add(Dense(6, activation='relu'))\n",
    "\n",
    "# 출력 레이어를 추가합니다. 3개의 노드와 'softmax' 활성화 함수를 사용합니다.\n",
    "model_val.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model_val.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약을 출력합니다.\n",
    "model_val.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 x_train 및 y_train을 사용하여 model_val을 훈련할 수 있습니다. 또한 x_test 및 y_test를 사용하여 각 에포크 후에 정확도를 테스트합니다. 아래 코드를 실행해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5356 - loss: 0.9983 - val_accuracy: 0.5000 - val_loss: 0.9872\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5965 - loss: 0.9682 - val_accuracy: 0.5333 - val_loss: 0.9755\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6071 - loss: 0.9527 - val_accuracy: 0.5333 - val_loss: 0.9639\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6400 - loss: 0.9410 - val_accuracy: 0.5333 - val_loss: 0.9523\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6444 - loss: 0.9271 - val_accuracy: 0.5333 - val_loss: 0.9408\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6108 - loss: 0.9116 - val_accuracy: 0.5333 - val_loss: 0.9293\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6702 - loss: 0.8887 - val_accuracy: 0.5667 - val_loss: 0.9182\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6844 - loss: 0.8764 - val_accuracy: 0.5667 - val_loss: 0.9073\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6710 - loss: 0.8615 - val_accuracy: 0.5667 - val_loss: 0.8965\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6210 - loss: 0.8931 - val_accuracy: 0.6000 - val_loss: 0.8861\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6731 - loss: 0.8571 - val_accuracy: 0.5667 - val_loss: 0.8759\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6898 - loss: 0.8322 - val_accuracy: 0.5667 - val_loss: 0.8661\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6627 - loss: 0.8287 - val_accuracy: 0.5667 - val_loss: 0.8565\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6992 - loss: 0.8118 - val_accuracy: 0.5667 - val_loss: 0.8474\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6804 - loss: 0.8084 - val_accuracy: 0.5667 - val_loss: 0.8383\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6544 - loss: 0.8089 - val_accuracy: 0.6000 - val_loss: 0.8292\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6700 - loss: 0.7900 - val_accuracy: 0.6000 - val_loss: 0.8203\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6796 - loss: 0.7812 - val_accuracy: 0.6000 - val_loss: 0.8116\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6879 - loss: 0.7613 - val_accuracy: 0.6000 - val_loss: 0.8030\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7317 - loss: 0.7234 - val_accuracy: 0.6000 - val_loss: 0.7945\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7015 - loss: 0.7302 - val_accuracy: 0.6000 - val_loss: 0.7860\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6837 - loss: 0.7468 - val_accuracy: 0.6000 - val_loss: 0.7777\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7096 - loss: 0.6954 - val_accuracy: 0.6000 - val_loss: 0.7696\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6950 - loss: 0.7050 - val_accuracy: 0.6000 - val_loss: 0.7615\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6950 - loss: 0.6772 - val_accuracy: 0.6000 - val_loss: 0.7535\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6835 - loss: 0.6897 - val_accuracy: 0.6000 - val_loss: 0.7457\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6835 - loss: 0.6704 - val_accuracy: 0.6000 - val_loss: 0.7379\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6767 - loss: 0.6886 - val_accuracy: 0.6000 - val_loss: 0.7303\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7121 - loss: 0.6535 - val_accuracy: 0.6000 - val_loss: 0.7228\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7048 - loss: 0.6492 - val_accuracy: 0.6333 - val_loss: 0.7156\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6923 - loss: 0.6482 - val_accuracy: 0.6333 - val_loss: 0.7083\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7048 - loss: 0.6220 - val_accuracy: 0.6333 - val_loss: 0.7012\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7100 - loss: 0.6118 - val_accuracy: 0.6333 - val_loss: 0.6941\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7060 - loss: 0.6116 - val_accuracy: 0.6333 - val_loss: 0.6871\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6750 - loss: 0.6420 - val_accuracy: 0.6333 - val_loss: 0.6802\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6754 - loss: 0.6490 - val_accuracy: 0.6333 - val_loss: 0.6735\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7108 - loss: 0.6187 - val_accuracy: 0.6333 - val_loss: 0.6669\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7713 - loss: 0.5736 - val_accuracy: 0.6333 - val_loss: 0.6603\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7402 - loss: 0.5716 - val_accuracy: 0.6333 - val_loss: 0.6536\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7267 - loss: 0.5688 - val_accuracy: 0.6333 - val_loss: 0.6472\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7548 - loss: 0.5423 - val_accuracy: 0.6667 - val_loss: 0.6408\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7360 - loss: 0.5593 - val_accuracy: 0.6667 - val_loss: 0.6342\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7454 - loss: 0.5541 - val_accuracy: 0.6667 - val_loss: 0.6275\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7394 - loss: 0.5672 - val_accuracy: 0.6667 - val_loss: 0.6209\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7415 - loss: 0.5453 - val_accuracy: 0.6667 - val_loss: 0.6147\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7217 - loss: 0.5521 - val_accuracy: 0.6667 - val_loss: 0.6083\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7698 - loss: 0.5167 - val_accuracy: 0.6667 - val_loss: 0.6021\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8033 - loss: 0.4948 - val_accuracy: 0.7000 - val_loss: 0.5959\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7294 - loss: 0.5413 - val_accuracy: 0.7000 - val_loss: 0.5893\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7460 - loss: 0.5146 - val_accuracy: 0.7000 - val_loss: 0.5827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28713129f90>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다. 이렇게 하면 모델이 학습할 수 있습니다.\n",
    "# Validation_data를 사용하면 테스트/검증 데이터 세트를 입력할 수 있습니다. 이를 통해 테스트/검증 세트에서 모델 정확도를 확인할 수 있습니다.\n",
    "model_val.fit(x_train2,y_train2,epochs=50,shuffle=True, validation_data=(x_test2,y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력에 검증 데이터의 정확도도 같이 표시되고 있습니다. 검증 정확도가 훈련 정확도보다 높은가요?아니면 낮은가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 모델을 사용하여 새로 수집된 데이터의 꽃 유형을 식별할 수 있습니다. 예를 들어, 친구가 일부 꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이 및 꽃잎 너비를 측정하고 \"iris_predict.data\"라는 파일에 데이터를 저장했다고 가정해 봅시다. 친구가 측정된 값을 기반으로 이 꽃의 종류를 찾고 싶어합니다. 친구를 돕기 위해 분류 모델을 사용하여 친구가 찾은 꽃의 종류를 찾아보세요. 아래 코드를 실행해 보세요!\n",
    "<font color=blue>힌트: model.predict 메서드를 사용하여 친구의 꽃 종류를 얻을 수 있습니다. 또한 .predict 메서드에서 반환된 값은 모델이 각 데이터 행에 할당해야 한다고 생각하는 각 꽃 유형의 확률입니다. 따라서 확률이 높을수록 모델이 예측한 꽃 유형이 정확하다는 확신을 가질 수 있습니다. 예를 들어 예측 값이 두 번째 열에서 매우 높으면 모델은 꽃 유형이 versicolor라고 생각할 수 있습니다. 또한 꽃 유형을 얻기 전에 데이터를 스케일링해야 합니다. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"[Dataset]_Module_18_(iris).data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\",\"class\"]\n",
    "df2.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = df2[['sepal_length','sepal_width','petal_length','petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_scale = standardise.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n"
     ]
    }
   ],
   "source": [
    "y_new = model_val.predict(x_new_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8487313  0.13447672 0.01679202]\n",
      " [0.90466064 0.08945057 0.0058888 ]\n",
      " [0.9128096  0.08215254 0.00503785]\n",
      " [0.9160609  0.07947981 0.00445932]\n",
      " [0.85147506 0.13219556 0.01632943]\n",
      " [0.7242076  0.21634652 0.05944586]\n",
      " [0.90051955 0.09274878 0.00673158]\n",
      " [0.86166066 0.12477059 0.01356871]\n",
      " [0.93984616 0.05802424 0.00212966]\n",
      " [0.8974508  0.09561995 0.00692926]\n",
      " [0.7832639  0.18040735 0.03632865]\n",
      " [0.8757948  0.11356518 0.01063997]\n",
      " [0.91532105 0.08009865 0.00458036]\n",
      " [0.9514808  0.04710425 0.00141486]\n",
      " [0.74665487 0.1985272  0.05481793]\n",
      " [0.70827633 0.22092512 0.07079854]\n",
      " [0.75863236 0.19476213 0.04660545]\n",
      " [0.842867   0.13903336 0.01809966]\n",
      " [0.71099275 0.22288904 0.06611826]\n",
      " [0.80218285 0.16788736 0.02992987]\n",
      " [0.7914717  0.1769488  0.03157943]\n",
      " [0.8066227  0.1652598  0.02811753]\n",
      " [0.9119652  0.08241412 0.0056207 ]\n",
      " [0.8257293  0.1532073  0.02106342]\n",
      " [0.8556254  0.13022798 0.01414657]\n",
      " [0.88312143 0.10827399 0.00860458]\n",
      " [0.84309244 0.1395756  0.01733184]\n",
      " [0.8280372  0.15020427 0.0217585 ]\n",
      " [0.84594446 0.13678889 0.01726663]\n",
      " [0.8983528  0.09489984 0.00674748]\n",
      " [0.8964135  0.09664043 0.00694602]\n",
      " [0.7961519  0.17337593 0.03047213]\n",
      " [0.7945147  0.17081797 0.03466737]\n",
      " [0.75098723 0.19614671 0.05286599]\n",
      " [0.8974508  0.09561995 0.00692926]\n",
      " [0.8947333  0.09759557 0.00767113]\n",
      " [0.8030491  0.16736998 0.02958095]\n",
      " [0.8974508  0.09561995 0.00692926]\n",
      " [0.93930364 0.05846511 0.00223128]\n",
      " [0.8503173  0.13363482 0.01604796]\n",
      " [0.8618843  0.12418048 0.01393521]\n",
      " [0.93665123 0.06206441 0.00128441]\n",
      " [0.9314433  0.06557056 0.00298614]\n",
      " [0.8203734  0.15654619 0.02308033]\n",
      " [0.75140196 0.20270765 0.04589041]\n",
      " [0.9084792  0.08616535 0.00535548]\n",
      " [0.7999025  0.16962558 0.0304719 ]\n",
      " [0.9152686  0.08006067 0.00467075]\n",
      " [0.7928689  0.1742728  0.03285829]\n",
      " [0.87618625 0.11309646 0.0107173 ]\n",
      " [0.18295756 0.3051631  0.5118794 ]\n",
      " [0.16981839 0.2808493  0.54933226]\n",
      " [0.16071558 0.27729672 0.5619877 ]\n",
      " [0.39609465 0.4453003  0.15860514]\n",
      " [0.18743198 0.31187618 0.5006919 ]\n",
      " [0.22177799 0.34561405 0.43260804]\n",
      " [0.13880534 0.24898407 0.61221063]\n",
      " [0.7304943  0.2391366  0.03036904]\n",
      " [0.2136587  0.3403044  0.44603685]\n",
      " [0.41267008 0.3597918  0.22753814]\n",
      " [0.663202   0.30791315 0.02888479]\n",
      " [0.18853955 0.2980545  0.5134059 ]\n",
      " [0.3312164  0.50132036 0.1674632 ]\n",
      " [0.17768876 0.28988332 0.53242797]\n",
      " [0.3776002  0.38690713 0.23549269]\n",
      " [0.20347422 0.32485628 0.47166944]\n",
      " [0.16193455 0.27231988 0.56574553]\n",
      " [0.3163725  0.43851826 0.2451092 ]\n",
      " [0.24906155 0.4083239  0.34261453]\n",
      " [0.39271525 0.44801003 0.15927467]\n",
      " [0.11570764 0.22220258 0.6620897 ]\n",
      " [0.25309038 0.37044472 0.376465  ]\n",
      " [0.17893371 0.3071609  0.5139053 ]\n",
      " [0.21221006 0.3327667  0.45502317]\n",
      " [0.23124354 0.35360402 0.41515243]\n",
      " [0.20724973 0.32989472 0.46285555]\n",
      " [0.19621061 0.32805955 0.47572985]\n",
      " [0.13363859 0.2432342  0.6231273 ]\n",
      " [0.17639178 0.28705633 0.5365519 ]\n",
      " [0.41990125 0.43422353 0.14587528]\n",
      " [0.44604975 0.43395168 0.11999853]\n",
      " [0.4675423  0.42599234 0.10646539]\n",
      " [0.3084758  0.43278885 0.25873536]\n",
      " [0.13790245 0.24798885 0.61410874]\n",
      " [0.17668319 0.2945999  0.5287169 ]\n",
      " [0.14802676 0.25892207 0.59305114]\n",
      " [0.16892861 0.28380698 0.5472645 ]\n",
      " [0.2792028  0.4257411  0.29505605]\n",
      " [0.23439205 0.34962848 0.4159795 ]\n",
      " [0.36542374 0.42645618 0.2081201 ]\n",
      " [0.30528113 0.41853932 0.27617946]\n",
      " [0.1790662  0.28952974 0.5314041 ]\n",
      " [0.30965918 0.4427406  0.24760021]\n",
      " [0.69709355 0.2700472  0.03285936]\n",
      " [0.27572787 0.39906362 0.3252085 ]\n",
      " [0.23968808 0.3512735  0.40903845]\n",
      " [0.2345362  0.35305077 0.41241303]\n",
      " [0.2249004  0.34348845 0.43161106]\n",
      " [0.7003139  0.26039225 0.03929376]\n",
      " [0.25849542 0.38507006 0.35643452]\n",
      " [0.03503323 0.0987179  0.8662489 ]\n",
      " [0.09408882 0.19438078 0.7115304 ]\n",
      " [0.06665636 0.15434247 0.7790012 ]\n",
      " [0.09128298 0.1905489  0.7181681 ]\n",
      " [0.05695606 0.13859163 0.80445236]\n",
      " [0.05115164 0.12867351 0.82017493]\n",
      " [0.36682895 0.32257116 0.31059983]\n",
      " [0.07792045 0.17151348 0.75056607]\n",
      " [0.10392772 0.20740262 0.6886697 ]\n",
      " [0.03210961 0.0928153  0.8750751 ]\n",
      " [0.08931544 0.18782909 0.72285545]\n",
      " [0.10571041 0.20969589 0.6845937 ]\n",
      " [0.07694252 0.17006594 0.75299144]\n",
      " [0.09956919 0.20156917 0.6988616 ]\n",
      " [0.05198911 0.13012946 0.8178814 ]\n",
      " [0.06114115 0.1455063  0.7933526 ]\n",
      " [0.09528252 0.19599466 0.70872283]\n",
      " [0.03126996 0.09108781 0.8776422 ]\n",
      " [0.04354617 0.11502875 0.84142506]\n",
      " [0.21313937 0.37149668 0.41536394]\n",
      " [0.0539865  0.13356727 0.81244624]\n",
      " [0.08306505 0.17842661 0.7385083 ]\n",
      " [0.05941594 0.1426787  0.7979053 ]\n",
      " [0.1344417  0.24413672 0.6214216 ]\n",
      " [0.06032301 0.14416929 0.79550767]\n",
      " [0.07847736 0.17233427 0.7491884 ]\n",
      " [0.13332099 0.2428764  0.6238027 ]\n",
      " [0.116988   0.2237624  0.6592496 ]\n",
      " [0.07387551 0.16547453 0.7606499 ]\n",
      " [0.11088292 0.2162404  0.67287666]\n",
      " [0.08315582 0.1791326  0.7377116 ]\n",
      " [0.04688084 0.12110876 0.83201045]\n",
      " [0.06733863 0.15541497 0.7772465 ]\n",
      " [0.15057763 0.26159984 0.58782244]\n",
      " [0.13742293 0.24745864 0.6151184 ]\n",
      " [0.05575533 0.13657224 0.80767244]\n",
      " [0.04661662 0.12063275 0.83275056]\n",
      " [0.09006087 0.18886274 0.72107637]\n",
      " [0.12026708 0.22771518 0.6520178 ]\n",
      " [0.07901081 0.17311822 0.7478709 ]\n",
      " [0.05185226 0.1298921  0.81825566]\n",
      " [0.07628356 0.16908614 0.7546303 ]\n",
      " [0.09408882 0.19438078 0.7115304 ]\n",
      " [0.04766625 0.12251808 0.8298156 ]\n",
      " [0.04120326 0.11065855 0.84813815]\n",
      " [0.07292422 0.16403419 0.76304156]\n",
      " [0.1239768  0.23228553 0.6437376 ]\n",
      " [0.09212706 0.19170739 0.71616554]\n",
      " [0.05637974 0.1376244  0.8059958 ]\n",
      " [0.10425994 0.20783147 0.68790853]]\n"
     ]
    }
   ],
   "source": [
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터 행의 꽃 유형은 무엇입니까? [argmax](https://www.geeksforgeeks.org/numpy-argmax-python/) 함수를 사용하여 y_new에서 꽃 종류를 식별할 수 있습니다. 아래 코드를 실행해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "flower_types = []\n",
    "for ii in range(0,y_new.shape[0]):\n",
    "    flower_types.append(np.argmax(y_new[ii,:]))\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-setosa Iris-virginica Iris-virginica Iris-virginica Iris-versicolor Iris-virginica Iris-virginica Iris-virginica Iris-setosa Iris-virginica Iris-setosa Iris-setosa Iris-virginica Iris-versicolor Iris-virginica Iris-versicolor Iris-virginica Iris-virginica Iris-versicolor Iris-versicolor Iris-versicolor Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-versicolor Iris-setosa Iris-setosa Iris-versicolor Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-versicolor Iris-virginica Iris-versicolor Iris-versicolor Iris-virginica Iris-versicolor Iris-setosa Iris-versicolor Iris-virginica Iris-virginica Iris-virginica Iris-setosa Iris-versicolor Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-setosa Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica Iris-virginica "
     ]
    }
   ],
   "source": [
    "names = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"]\n",
    "for i in range(len(flower_types)) :\n",
    "    print(names[flower_types[i]], end= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번호로 표기된 꽃의 유형을 다시 원래의 클래스 정보(Setosa, Versicolor, Virginica)로 변경할 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# 클래스 정보를 다시 숫자로 변환하는 딕셔너리\n",
    "class_encode = {0: \"Iris-setosa\", 1: \"Iris-versicolor\", 2: \"Iris-virginica\"}\n",
    "\n",
    "# 원핫인코딩된 결과를 클래스 정보로 변환\n",
    "y_class_original = [class_encode[np.argmax(x)] for x in y_values]\n",
    "\n",
    "# 결과 출력\n",
    "print(y_class_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 인공 신경망 모델을 만들고 훈련하는 방법을 성공적으로 배웠습니다. 인공 신경망은 매우 큰 데이터 세트에 사용할 수 있는 매우 강력한 도구입니다. 예를 들어, 수백 개의 열/특성과 100,000개 이상의 데이터 포인트가 있는 경우 다른 기계 학습 기술 대신 인공 신경망을 사용하는 것이 유리할 수 있습니다. 은닉 레이어 노드 수 또는 은닉 레이어 수에 대한 엄격한 규칙은 없습니다. 다양한 신경망을 실험하여 데이터 세트에 가장 적합한 결과를 제공하는 신경망을 찾는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gadi2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m88\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m27\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> (660.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m165\u001b[0m (660.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> (660.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m165\u001b[0m (660.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chat GPT로 인공신경망 처음부터 끝까지 해봄\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 데이터 불러오기\n",
    "# pandas 라이브러리를 사용하여 CSV 파일에서 데이터를 읽어옵니다.\n",
    "# CSV 파일의 첫 번째 행은 열 이름이 아니므로, header=None으로 지정합니다.\n",
    "df = pd.read_csv(\"[Dataset]_Module_18_(iris).data\", header=None)\n",
    "\n",
    "# 컬럼 이름 지정\n",
    "# 데이터프레임의 열 이름을 지정합니다.\n",
    "names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = names\n",
    "\n",
    "# 입력과 타겟 데이터로 분리\n",
    "# 'class' 열을 제외한 나머지 열은 입력 데이터(X)가 되고, 'class' 열은 타겟 데이터(y)가 됩니다.\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# 원핫인코딩\n",
    "# sklearn의 OneHotEncoder를 사용하여 범주형 변수인 타겟 데이터(y)를 원핫인코딩합니다.\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(y.values.reshape(-1,1)).toarray()\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 분리\n",
    "# 데이터를 학습 데이터와 테스트 데이터로 나누어 모델을 학습시키고 평가합니다.\n",
    "# test_size=0.2로 지정하여 전체 데이터의 20%를 테스트 데이터로 사용합니다.\n",
    "# random_state=42로 지정하여 데이터를 무작위로 섞을 때 사용되는 시드를 고정합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 구성\n",
    "# Sequential 모델을 사용하여 인공신경망 모델을 구성합니다.\n",
    "# Dense 레이어는 fully connected layer를 의미하며, 각 레이어의 노드 수와 활성화 함수를 지정할 수 있습니다.\n",
    "# input_shape=(4,)로 지정하여 입력 데이터의 형태를 설정합니다. 여기서 4는 입력 데이터의 특성 개수입니다.\n",
    "model = models.Sequential([\n",
    "    layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 출력 레이어의 노드 수를 클래스 개수에 맞게 변경\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "# 모델을 컴파일하여 학습 과정을 설정합니다.\n",
    "# optimizer는 'adam'을 사용하며, 손실 함수는 'categorical_crossentropy'를 사용합니다.\n",
    "# metrics는 모델의 성능 평가 지표로 'accuracy'를 사용합니다.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # categorical_crossentropy로 변경\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "# 모델의 구조와 파라미터 개수 등을 요약하여 출력합니다.\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "# 모델을 학습시킵니다. 입력 데이터와 타겟 데이터를 사용하여 모델을 학습합니다.\n",
    "# epochs=100으로 지정하여 전체 데이터셋을 100번 반복하여 학습합니다.\n",
    "# validation_data=(X_test, y_test)로 지정하여 학습 도중에 테스트 데이터를 사용하여 검증합니다.\n",
    "# verbose=0으로 지정하여 학습 과정을 출력하지 않습니다.\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# 학습 결과 시각화\n",
    "# 학습 과정에서의 정확도를 시각화하여 모델의 성능을 확인합니다.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
