{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-discretion",
   "metadata": {
    "id": "artificial-discretion"
   },
   "source": [
    "# Working with Open Model Zoo Models\n",
    "\n",
    "> **Note:** This notebook has been moved to a new branch named \"latest\". [Click here](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/model-tools/model-tools.ipynb) to get the most updated version of the notebook. This branch is deprecated.\n",
    "This tutorial shows how to download a model from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), convert it to OpenVINO™ IR format, show information about the model, and benchmark the model.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [OpenVINO and Open Model Zoo Tools](#OpenVINO-and-Open-Model-Zoo-Tools)\n",
    "- [Preparation](#Preparation)\n",
    "    - [Model Name](#Model-Name)\n",
    "    - [Imports](#Imports)\n",
    "    - [Settings and Configuration](#Settings-and-Configuration)\n",
    "- [Download a Model from Open Model Zoo](#Download-a-Model-from-Open-Model-Zoo)\n",
    "- [Convert a Model to OpenVINO IR format](#Convert-a-Model-to-OpenVINO-IR-format)\n",
    "- [Get Model Information](#Get-Model-Information)\n",
    "- [Run Benchmark Tool](#Run-Benchmark-Tool)\n",
    "    - [Benchmark with Different Settings](#Benchmark-with-Different-Settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dda33e",
   "metadata": {
    "id": "c4dda33e"
   },
   "source": [
    "## OpenVINO and Open Model Zoo Tools\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "OpenVINO and Open Model Zoo tools are listed in the table below.\n",
    "\n",
    "| Tool             | Command             | Description                                             |\n",
    "|:-----------------|:--------------------|:--------------------------------------------------------|\n",
    "| Model Downloader | `omz_downloader`      | Download models from Open Model Zoo.                    |\n",
    "| Model Converter  | `omz_converter`       | Convert Open Model Zoo models to OpenVINO's IR format.  |\n",
    "| Info Dumper      | `omz_info_dumper`     | Print information about Open Model Zoo models.          |\n",
    "| Benchmark Tool   | `benchmark_app`       | Benchmark model performance by computing inference time.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57288459",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57288459",
    "outputId": "2d232b5b-e66a-4f5e-bf39-a7fbdcbc01f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
      "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install openvino package\n",
    "%pip install -q \"openvino-dev>=2024.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec90f61-08f3-4417-8a6f-ec61c9e5955b",
   "metadata": {
    "id": "fec90f61-08f3-4417-8a6f-ec61c9e5955b",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Preparation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Model Name\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Set `model_name` to the name of the Open Model Zoo model to use in this notebook. Refer to the list of [public](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/index.md) and [Intel](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/index.md) pre-trained models for a full list of models that can be used. Set `model_name` to the model you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6d2be-05a8-4c52-9ad9-af892a76db1f",
   "metadata": {
    "id": "21d6d2be-05a8-4c52-9ad9-af892a76db1f"
   },
   "outputs": [],
   "source": [
    "# model_name = \"resnet-50-pytorch\"\n",
    "model_name = \"mobilenet-v2-pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eQEl_MdDaWM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eQEl_MdDaWM",
    "outputId": "0c841492-b2b5-49b8-ea6d-987ea183804f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0206a22-dc33-4666-ab2b-86386b97caca",
   "metadata": {
    "id": "f0206a22-dc33-4666-ab2b-86386b97caca",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-uncertainty",
   "metadata": {
    "id": "impressed-uncertainty"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import openvino as ov\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "from notebook_utils import DeviceNotFoundAlert, NotebookAlert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-assets",
   "metadata": {
    "id": "parental-assets",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Settings and Configuration\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Set the file and directory paths. By default, this notebook downloads models from Open Model Zoo to the `open_model_zoo_models` directory in your `$HOME` directory. On Windows, the $HOME directory is usually `c:\\users\\username`, on Linux `/home/username`. To change the folder, change `base_model_dir` in the cell below.\n",
    "\n",
    "The following settings can be changed:\n",
    "\n",
    "* `base_model_dir`: Models will be downloaded into the `intel` and `public` folders in this directory.\n",
    "* `omz_cache_dir`: Cache folder for Open Model Zoo. Specifying a cache directory is not required for Model Downloader and Model Converter, but it speeds up subsequent downloads.\n",
    "* `precision`: If specified, only models with this precision will be downloaded and converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-agency",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "korean-agency",
    "outputId": "0a41af81-1c38-4a64-9d16-b6dd1af0e7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model_dir: model, omz_cache_dir: cache, gpu_availble: False\n"
     ]
    }
   ],
   "source": [
    "base_model_dir = Path(\"model\")\n",
    "omz_cache_dir = Path(\"cache\")\n",
    "precision = \"FP16\"\n",
    "\n",
    "# Check if an GPU is available on this system to use with Benchmark App.\n",
    "core = ov.Core()\n",
    "gpu_available = \"GPU\" in core.available_devices\n",
    "\n",
    "print(\n",
    "    f\"base_model_dir: {base_model_dir}, omz_cache_dir: {omz_cache_dir}, gpu_availble: {gpu_available}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-preview",
   "metadata": {
    "id": "judicial-preview"
   },
   "source": [
    "## Download a Model from Open Model Zoo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-interval",
   "metadata": {
    "id": "rising-interval"
   },
   "source": [
    "Specify, display and run the Model Downloader command to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b0446-0c49-41cf-9f3f-dec1232249f7",
   "metadata": {
    "id": "df2b0446-0c49-41cf-9f3f-dec1232249f7"
   },
   "outputs": [],
   "source": [
    "## Uncomment the next line to show help in omz_downloader which explains the command-line options.\n",
    "\n",
    "# !omz_downloader --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AJdrS2m9_32U",
   "metadata": {
    "id": "AJdrS2m9_32U"
   },
   "source": [
    "$download_command 명령어는 OpenVINO™ Toolkit의 일부로 제공되는 Model Downloader (omz_downloader)를 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d0c12-15cf-492d-a1ed-41dff5090eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "556d0c12-15cf-492d-a1ed-41dff5090eff",
    "outputId": "271fc71c-c934-4b06-e8c1-ade184ff5c75"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Download command: `omz_downloader --name mobilenet-v2-pytorch --output_dir model --cache_dir cache`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Downloading mobilenet-v2-pytorch..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading mobilenet-v2-pytorch ||################\n",
      "\n",
      "========== Retrieving model/public/mobilenet-v2-pytorch/mobilenet_v2-b0353104.pth from the cache\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_command = (\n",
    "    f\"omz_downloader --name {model_name} --output_dir {base_model_dir} --cache_dir {omz_cache_dir}\"\n",
    ")\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name}...\"))\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-checklist",
   "metadata": {
    "id": "proprietary-checklist"
   },
   "source": [
    "## Convert a Model to OpenVINO IR format\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Specify, display and run the Model Converter command to convert the model to OpenVINO IR format. Model conversion may take a while. The output of the Model Converter command will be displayed. When the conversion is successful, the last lines of the output will include: `[ SUCCESS ] Generated IR version 11 model.` For downloaded models that are already in OpenVINO IR format, conversion will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe7461-90db-4585-b55f-b3df42b01274",
   "metadata": {
    "id": "11fe7461-90db-4585-b55f-b3df42b01274"
   },
   "outputs": [],
   "source": [
    "## Uncomment the next line to show Help in omz_converter which explains the command-line options.\n",
    "\n",
    "# !omz_converter --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EvF50ZtMEl4Z",
   "metadata": {
    "id": "EvF50ZtMEl4Z"
   },
   "source": [
    "PyTorch에서 구현된 MobileNetV2 모델을 ONNX(Open Neural Network Exchange) 형식으로 변환하는 것은 여러 가지 면에서 의미가 있습니다. ONNX는 다양한 플랫폼과 프레임워크 간의 모델 호환성을 제공하여, 다양한 환경에서 모델을 쉽게 배포하고 실행할 수 있게 해줍니다. MobileNetV2를 ONNX로 변환하는 것이 가지는 주요 이점을 설명드리겠습니다:\n",
    "\n",
    "1. 플랫폼 및 프레임워크 독립성\n",
    "\n",
    "ONNX는 모델을 다양한 머신러닝 프레임워크에서 사용할 수 있도록 지원합니다. 예를 들어, PyTorch에서 훈련된 모델을 ONNX로 변환하면, TensorFlow, Microsoft Cognitive Toolkit (CNTK), Apple CoreML 등 다른 프레임워크에서도 사용할 수 있습니다. 이는 개발자가 모델을 더 넓은 범위의 애플리케이션과 시스템에 적용할 수 있게 합니다.\n",
    "\n",
    "2. 배포 용이성\n",
    "\n",
    "ONNX 모델은 클라우드, 모바일, 임베디드 시스템과 같이 다양한 환경에서 실행될 수 있습니다. 이는 특히 IoT 디바이스나 모바일 애플리케이션 같이 컴퓨팅 자원이 제한된 환경에서 모델을 배포할 때 큰 장점이 됩니다.\n",
    "\n",
    "3. 성능 최적화\n",
    "\n",
    "ONNX는 하드웨어 가속화를 지원하는 다양한 최적화를 제공할 수 있습니다. ONNX 모델은 특정 하드웨어(예: GPU, FPGA)에서 실행할 때 성능을 최적화하기 위해 다양한 도구와 라이브러리를 사용할 수 있습니다. 이를 통해 실행 속도와 효율성이 크게 향상될 수 있습니다.\n",
    "\n",
    "4. 유지 관리 및 업데이트의 용이성\n",
    "\n",
    "ONNX 형식으로 모델을 표준화하면, 모델의 버전 관리와 업데이트가 더욱 용이해집니다. 모델을 쉽게 업데이트하고 배포할 수 있으며, 호환성 문제 없이 다양한 환경에서 일관된 성능을 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-academy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "engaged-academy",
    "outputId": "dbdbd6f1-adb1-4d8b-8e35-99097f729b8b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Convert command: `omz_converter --name mobilenet-v2-pytorch --precisions FP16 --download_dir model --output_dir model`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Converting mobilenet-v2-pytorch..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Converting mobilenet-v2-pytorch to ONNX\n",
      "Conversion to ONNX command: /usr/bin/python3 -- /usr/local/lib/python3.10/dist-packages/omz_tools/internal_scripts/pytorch_to_onnx.py --model-name=mobilenet_v2 --weights=model/public/mobilenet-v2-pytorch/mobilenet_v2-b0353104.pth --import-module=torchvision.models --input-shape=1,3,224,224 --output-file=model/public/mobilenet-v2-pytorch/mobilenet-v2.onnx --input-names=data --output-names=prob\n",
      "\n",
      "ONNX check passed successfully.\n",
      "\n",
      "========== Converting mobilenet-v2-pytorch to IR (FP16)\n",
      "Conversion command: /usr/bin/python3 -- /usr/local/bin/mo --framework=onnx --output_dir=model/public/mobilenet-v2-pytorch/FP16 --model_name=mobilenet-v2-pytorch --input=data '--mean_values=data[123.675,116.28,103.53]' '--scale_values=data[58.624,57.12,57.375]' --reverse_input_channels --output=prob --input_model=model/public/mobilenet-v2-pytorch/mobilenet-v2.onnx '--layout=data(NCHW)' '--input_shape=[1, 3, 224, 224]' --compress_to_fp16=True\n",
      "\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release. Please use OpenVINO Model Converter (OVC). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /content/model/public/mobilenet-v2-pytorch/FP16/mobilenet-v2-pytorch.xml\n",
      "[ SUCCESS ] BIN file: /content/model/public/mobilenet-v2-pytorch/FP16/mobilenet-v2-pytorch.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {base_model_dir} --output_dir {base_model_dir}\"\n",
    "display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "display(Markdown(f\"Converting {model_name}...\"))\n",
    "\n",
    "! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d655f-215d-4e3c-adcb-e8fd4a2e8ab4",
   "metadata": {
    "id": "aa8d655f-215d-4e3c-adcb-e8fd4a2e8ab4"
   },
   "source": [
    "## Get Model Information\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The Info Dumper prints the following information for Open Model Zoo models:\n",
    "\n",
    "* Model name\n",
    "* Description\n",
    "* Framework that was used to train the model\n",
    "* License URL\n",
    "* Precisions supported by the model\n",
    "* Subdirectory: the location of the downloaded model\n",
    "* Task type\n",
    "\n",
    "This information can be shown by running `omz_info_dumper --name model_name` in a terminal. The information can also be parsed and used in scripts.\n",
    "\n",
    "In the next cell, run Info Dumper and use `json` to load the information in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xUW_WNEQBQIY",
   "metadata": {
    "id": "xUW_WNEQBQIY"
   },
   "source": [
    "json.loads():\n",
    "\n",
    "json 모듈은 Python에서 JSON 데이터를 다룰 때 사용합니다.\n",
    "loads() 함수는 JSON 형식의 문자열을 입력으로 받아, 그 내용을 Python의 데이터 타입(예: 딕셔너리, 리스트)으로 변환합니다. loads는 \"load string\"의 줄임말입니다.\n",
    "\n",
    "model_info_output.get_nlstr():\n",
    "\n",
    "이 부분은 model_info_output 객체의 get_nlstr() 메서드를 호출하는 것으로 보입니다. get_nlstr() 메서드의 정확한 기능은 model_info_output 객체의 정의에 따라 다르지만, 일반적으로 객체에서 특정 데이터를 문자열 형태로 추출하는 메서드일 가능성이 높습니다.\n",
    "여기서 get_nlstr()는 JSON 형식의 문자열을 반환할 것으로 예상됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8247daf-d3c5-4420-b4c8-d305ac4ace5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8247daf-d3c5-4420-b4c8-d305ac4ace5b",
    "outputId": "0e89973d-bb2a-4528-d05f-f02efab30ff7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'mobilenet-v2-pytorch',\n",
       "  'composite_model_name': None,\n",
       "  'description': 'MobileNet V2 is image classification model pre-trained on ImageNet dataset. This is a PyTorch* implementation of MobileNetV2 architecture as described in the paper \"Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation\" <https://arxiv.org/abs/1801.04381>.\\nThe model input is a blob that consists of a single image of \"1, 3, 224, 224\" in \"RGB\" order.\\nThe model output is typical object classifier for the 1000 different classifications matching with those in the ImageNet database.',\n",
       "  'framework': 'pytorch',\n",
       "  'license_url': 'https://raw.githubusercontent.com/pytorch/vision/master/LICENSE',\n",
       "  'accuracy_config': '/usr/local/lib/python3.10/dist-packages/omz_tools/models/public/mobilenet-v2-pytorch/accuracy-check.yml',\n",
       "  'model_config': '/usr/local/lib/python3.10/dist-packages/omz_tools/models/public/mobilenet-v2-pytorch/model.yml',\n",
       "  'precisions': ['FP16', 'FP32'],\n",
       "  'subdirectory': 'public/mobilenet-v2-pytorch',\n",
       "  'task_type': 'classification',\n",
       "  'input_info': [{'name': 'data',\n",
       "    'shape': [1, 3, 224, 224],\n",
       "    'layout': 'NCHW'}],\n",
       "  'model_stages': []}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info_output = %sx omz_info_dumper --name $model_name\n",
    "model_info = json.loads(model_info_output.get_nlstr())\n",
    "\n",
    "if len(model_info) > 1:\n",
    "    NotebookAlert(\n",
    "        f\"There are multiple IR files for the {model_name} model. The first model in the \"\n",
    "        \"omz_info_dumper output will be used for benchmarking. Change \"\n",
    "        \"`selected_model_info` in the cell below to select a different model from the list.\",\n",
    "        \"warning\",\n",
    "    )\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e868-fd2d-4d11-9c87-7aa1f1301083",
   "metadata": {
    "id": "7ea7e868-fd2d-4d11-9c87-7aa1f1301083"
   },
   "source": [
    "Having information of the model in a JSON file enables extraction of the path to the model directory, and building the path to the OpenVINO IR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a319e-bbef-414c-921d-60938b4a01a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de1a319e-bbef-414c-921d-60938b4a01a8",
    "outputId": "dc9a6536-0c81-49e8-fca7-681770402c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/public/mobilenet-v2-pytorch/FP16/mobilenet-v2-pytorch.xml exists: True\n"
     ]
    }
   ],
   "source": [
    "selected_model_info = model_info[0]\n",
    "model_path = (\n",
    "    base_model_dir\n",
    "    / Path(selected_model_info[\"subdirectory\"])\n",
    "    / Path(f\"{precision}/{selected_model_info['name']}.xml\")\n",
    ")\n",
    "print(model_path, \"exists:\", model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e01154-f700-479f-9111-147c95595d46",
   "metadata": {
    "id": "54e01154-f700-479f-9111-147c95595d46"
   },
   "source": [
    "## Run Benchmark Tool\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "By default, Benchmark Tool runs inference for 60 seconds in asynchronous mode on CPU. It returns inference speed as latency (milliseconds per image) and throughput values (frames per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282452e8-24c7-49c0-bdb2-10677971c30f",
   "metadata": {
    "id": "282452e8-24c7-49c0-bdb2-10677971c30f"
   },
   "outputs": [],
   "source": [
    "## Uncomment the next line to show Help in benchmark_app which explains the command-line options.\n",
    "# !benchmark_app --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812b0c8-8cd0-4840-bca3-a28171d055b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9812b0c8-8cd0-4840-bca3-a28171d055b7",
    "outputId": "eef3a736-af49-4f1f-92b9-fe6285fb5df3",
    "tags": [],
    "test_replace": {
     "-t 15": "-t 3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Benchmark command: `benchmark_app -m model/public/mobilenet-v2-pytorch/FP16/mobilenet-v2-pytorch.xml -t 15`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Benchmarking mobilenet-v2-pytorch on CPU with async inference for 15 seconds..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2024.1.0-15008-f4afc983258-releases/2024/1\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2024.1.0-15008-f4afc983258-releases/2024/1\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to PerformanceMode.THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 18.20 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     data (node: data) : f32 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     prob (node: prob) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     data (node: data) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     prob (node: prob) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "mbind failed: Operation not permitted\n",
      "mbind failed: Operation not permitted\n",
      "[ INFO ] Compile model took 274.49 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: main_graph\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
      "[ INFO ]   NUM_STREAMS: 2\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 2\n",
      "[ INFO ]   PERF_COUNT: NO\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: THROUGHPUT\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: True\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   MODEL_DISTRIBUTION_POLICY: set()\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[ INFO ]   CPU_DENORMALS_OPTIMIZATION: False\n",
      "[ INFO ]   LOG_LEVEL: Level.NO\n",
      "[ INFO ]   CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "[ INFO ]   DYNAMIC_QUANTIZATION_GROUP_SIZE: 0\n",
      "[ INFO ]   KV_CACHE_PRECISION: <Type: 'float16'>\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'data'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'data' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 2 inference requests, limits: 15000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 25.05 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            1074 iterations\n",
      "[ INFO ] Duration:         15015.59 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        21.66 ms\n",
      "[ INFO ]    Average:       27.76 ms\n",
      "[ INFO ]    Min:           13.36 ms\n",
      "[ INFO ]    Max:           154.06 ms\n",
      "[ INFO ] Throughput:   71.53 FPS\n"
     ]
    }
   ],
   "source": [
    "benchmark_command = f\"benchmark_app -m {model_path} -t 15\"\n",
    "display(Markdown(f\"Benchmark command: `{benchmark_command}`\"))\n",
    "display(Markdown(f\"Benchmarking {model_name} on CPU with async inference for 15 seconds...\"))\n",
    "\n",
    "! $benchmark_command"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "openvino_notebooks": {
   "imageUrl": "",
   "tags": {
    "categories": [
     "API Overview",
     "Convert"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Classification"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
